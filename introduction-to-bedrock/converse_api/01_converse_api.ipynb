{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> How to work with Converse API in Amazon Bedrock - Getting Started. </h2>\n",
    "\n",
    "*Note: This notebook has been adapted from the [Getting started with the Converse API in Amazon Bedrock](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/introduction-to-bedrock/Getting_started_with_Converse_API.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Overview </h2>\n",
    "\n",
    "In this notebook, we'll explore the basics of the Converse API in Amazon Bedrock. The Converse or ConverseStream API is a unified structured text API action that allows you simplifying the invocations to Bedrock LLMs, using a universal syntax and message structured prompts for any of the supported model providers.\n",
    "\n",
    "To use the Converse API, you call the `Converse` or `ConverseStream` operations to send messages to a model. To call Converse, you require permission for the `bedrock:InvokeModel` operation. To call ConverseStream, you require permission for the `bedrock:InvokeModelWithResponseStream` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites </h2>\n",
    "\n",
    "Before you can use Amazon Bedrock, you must carry out the following steps:\n",
    "\n",
    "- Sign up for an AWS account (if you don't already have one) and IAM Role with the necessary permissions for Amazon Bedrock, see [AWS Account and IAM Role](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#new-to-aws).\n",
    "- Request access to the foundation models (FM) that you want to use, see [Request access to FMs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access). \n",
    "    \n",
    "    We have used below Foundation Models in our examples in this Notebook in `us-east-1` (North Virginia) region.\n",
    "      \n",
    "| Provider Name | Foundation Model Name | Model Id |\n",
    "| ------- | ------------- | ------------- |\n",
    "| AI21 Labs | Jamba 1.5 Large | ai21.jamba-1-5-large-v1:0 |\n",
    "| AI21 Labs | Jamba-Instruct  | ai21.jamba-instruct-v1:0 |\n",
    "| Amazon | Nova Pro  | amazon.nova-pro-v1:0 |\n",
    "| Amazon | Nova Lite  | amazon.nova-lite-v1:0 |\n",
    "| Amazon | Nova Micro  | amazon.nova-micro-v1:0 |\n",
    "| Anthropic | Claude 3.5 Sonnet  | anthropic.claude-3-5-sonnet-20240620-v1:0 |\n",
    "| Anthropic | Claude 3 Haiku  | anthropic.claude-3-haiku-20240307-v1:0 |\n",
    "| Cohere | Command R+ | cohere.command-r-plus-v1:0 |\n",
    "| Cohere | Command R | cohere.command-r-v1:0 |\n",
    "| Meta | Llama 3.1 70B Instruct | meta.llama3-1-70b-instruct-v1:0 |\n",
    "| Meta | Llama 3.1 8B Instruct | meta.llama3-1-8b-instruct-v1:0 |\n",
    "| Mistral AI | Mistral Large 2 (24.07) | mistral.mistral-large-2407-v1:0 |\n",
    "| Mistral AI | Mixtral 8X7B Instruct | mistral.mixtral-8x7b-instruct-v0:1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Setup </h2>\n",
    "\n",
    "⚠️ This notebook should work well with the Data Science 3.0 kernel (Python 3.10 runtime) in SageMaker Studio ⚠️\n",
    "\n",
    "Run the cells in this section to install the packages needed by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.37.10-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.38.0,>=1.37.10 (from boto3)\n",
      "  Downloading botocore-1.37.10-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
      "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.38.0,>=1.37.10->boto3)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.38.0,>=1.37.10->boto3)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.10->boto3)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading boto3-1.37.10-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.37.10-py3-none-any.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: urllib3, six, jmespath, python-dateutil, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.20\n",
      "    Uninstalling urllib3-1.26.20:\n",
      "      Successfully uninstalled urllib3-1.26.20\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.37.10\n",
      "    Uninstalling botocore-1.37.10:\n",
      "      Successfully uninstalled botocore-1.37.10\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.11.4\n",
      "    Uninstalling s3transfer-0.11.4:\n",
      "      Successfully uninstalled s3transfer-0.11.4\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.37.10\n",
      "    Uninstalling boto3-1.37.10:\n",
      "      Successfully uninstalled boto3-1.37.10\n",
      "Successfully installed boto3-1.37.10 botocore-1.37.10 jmespath-1.0.1 python-dateutil-2.9.0.post0 s3transfer-0.11.4 six-1.17.0 urllib3-1.26.20\n",
      "Running boto3 version: 1.37.10\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall boto3\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "from botocore.exceptions import ClientError\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the region and models to use. We can also setup our boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using region:  us-east-1\n"
     ]
    }
   ],
   "source": [
    "region = 'us-east-1'\n",
    "print('Using region: ', region)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )\n",
    "\n",
    "#I renamed the original list as I don't want to test many FM models at once\n",
    "ORIGINAL_MODEL_IDS = [\n",
    "    \"ai21.jamba-1-5-large-v1:0\",\n",
    "    \"ai21.jamba-instruct-v1:0\",\n",
    "    \"amazon.nova-pro-v1:0\",\n",
    "    \"amazon.nova-lite-v1:0\",\n",
    "    \"amazon.nova-micro-v1:0\",\n",
    "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"cohere.command-r-plus-v1:0\",\n",
    "    \"cohere.command-r-v1:0\",\n",
    "    \"meta.llama3-70b-instruct-v1:0\",\n",
    "    \"meta.llama3-8b-instruct-v1:0\",\n",
    "    \"mistral.mistral-large-2402-v1:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "    ]\n",
    "\n",
    "MODEL_IDS = [\n",
    "    \"amazon.nova-micro-v1:0\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Notebook/Code with comments</h2>\n",
    "\n",
    "We're now ready to setup our Converse API action in Bedrock. Note that we use the same syntax for any model, including the messages-formatted prompts, and the inference parameters. Also note that we read the output in the same way independently of the model used.\n",
    "\n",
    "Optionally, we could define additional model specific request fields that are not common across all providers. For more information on this check the [Bedrock Converse API documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n",
    "\n",
    "\n",
    "<h3> Converse for one-shot invocations </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_bedrock_model(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    response = \"\"\n",
    "    try:\n",
    "        response = client.converse(\n",
    "            modelId=id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": temperature,\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "            #additionalModelRequestFields={\n",
    "            #}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Model invocation error\"\n",
    "    try:\n",
    "        result = response['output']['message']['content'][0]['text'] \\\n",
    "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
    "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
    "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Output parsing error\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test our invocation.\n",
    "\n",
    "In this example, we run the same prompt across all the text models supported in Bedrock by the time of writing this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of Italy? Limit your response to 10 words only\n",
      "\n",
      "Model: amazon.nova-micro-v1:0\n",
      "Rome is capital of Italy.\n",
      "--- Latency: 148ms - Input tokens:16 - Output tokens:6 ---\n",
      "\n",
      "Model: anthropic.claude-3-haiku-20240307-v1:0\n",
      "The capital of Italy is Rome.\n",
      "--- Latency: 784ms - Input tokens:24 - Output tokens:10 ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = (\"What is the capital of Italy? Limit your response to 10 words only\")\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "\n",
    "for i in MODEL_IDS:\n",
    "    response = invoke_bedrock_model(bedrock, i, prompt)\n",
    "    print(f'Model: {i}\\n{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> ConverseStream for streaming invocations </h3>\n",
    "\n",
    "We can also use the Converse API for streaming invocations. In this case we rely on the ConverseStream action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_bedrock_model_stream(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    response = \"\"\n",
    "    response = client.converse_stream(\n",
    "        modelId=id,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p\n",
    "        }\n",
    "    )\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            chunk = event['contentBlockDelta']\n",
    "            sys.stdout.write(chunk['delta']['text'])\n",
    "            sys.stdout.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of Italy? Limit your response to 50 words maximum\n",
      "\n",
      "\n",
      "\n",
      "Model: amazon.nova-micro-v1:0\n",
      "The capital of Italy is Rome. It is a historic city known for landmarks like the Colosseum, the Vatican City, and the Roman Forum, and it serves as the political and cultural heart of the country.\n",
      "\n",
      "Model: anthropic.claude-3-haiku-20240307-v1:0\n",
      "The capital of Italy is Rome. Rome has been the capital of Italy since the unification of the country in 1861."
     ]
    }
   ],
   "source": [
    "prompt = (\"What is the capital of Italy? Limit your response to 50 words maximum\")\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "\n",
    "for i in MODEL_IDS:\n",
    "    print(f'\\n\\nModel: {i}')\n",
    "    invoke_bedrock_model_stream(bedrock, i, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Conversation with Text using Converse API and model specific parameters </h3>\n",
    "\n",
    "In this example we will call the Converse operation with the Anthropic Claude 3 Haiku model. We will send the input text, inference parameters, and additional parameters that are unique to the model. We will start a conversation by asking the model to create a list of songs, then continues the conversation by asking that the songs are by artists from the United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_conversation(bedrock_client,\n",
    "                          model_id,\n",
    "                          system_prompts,\n",
    "                          messages):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompts (JSON) : The system prompts for the model to use.\n",
    "        messages (JSON) : The messages to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Generating message with model {model_id}')\n",
    "\n",
    "    # Inference parameters to use.\n",
    "    temperature = 0.5\n",
    "    top_k = 200\n",
    "\n",
    "    # Base inference parameters to use which are common across all FMs.\n",
    "    inference_config = {\"temperature\": temperature}\n",
    "\n",
    "    # Additional inference parameters to use for Anthropic Claude Models.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    # Log token usage.\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens: {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens: {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens: {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model anthropic.claude-3-haiku-20240307-v1:0\n",
      "Input tokens: 45\n",
      "Output tokens: 55\n",
      "Total tokens: 100\n",
      "Stop reason: end_turn\n",
      "Generating message with model anthropic.claude-3-haiku-20240307-v1:0\n",
      "Input tokens: 115\n",
      "Output tokens: 60\n",
      "Total tokens: 175\n",
      "Stop reason: end_turn\n",
      "Role: user\n",
      "Text: Create a list of 3 pop songs.\n",
      "\n",
      "Role: assistant\n",
      "Text: Here are 3 pop songs with the artist:\n",
      "\n",
      "1. \"Levitating\" by Dua Lipa\n",
      "2. \"Drivers License\" by Olivia Rodrigo\n",
      "3. \"Butter\" by BTS\n",
      "\n",
      "Role: user\n",
      "Text: Make sure the songs are by artists from the United Kingdom.\n",
      "\n",
      "Role: assistant\n",
      "Text: Here are 3 pop songs by artists from the United Kingdom:\n",
      "\n",
      "1. \"Don't Start Now\" by Dua Lipa\n",
      "2. \"Watermelon Sugar\" by Harry Styles\n",
      "3. \"Physical\" by Dua Lipa\n",
      "\n",
      "Finished generating text with model anthropic.claude-3-haiku-20240307-v1:0.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Setup the system prompts and messages to send to the model.\n",
    "system_prompts = [{\"text\": \"You are an app that creates playlists for a radio station that plays rock and pop music.\"\n",
    "                    \"Only return song names and the artist.\"}]\n",
    "message_1 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Create a list of 3 pop songs.\"}]\n",
    "}\n",
    "message_2 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Make sure the songs are by artists from the United Kingdom.\"}]\n",
    "}\n",
    "messages = []\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    # Start the conversation with the 1st message.\n",
    "    messages.append(message_1)\n",
    "    response = generate_conversation(\n",
    "        bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "    # Add the response message to the conversation.\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "\n",
    "    # Continue the conversation with the 2nd message.\n",
    "    messages.append(message_2)\n",
    "    response = generate_conversation(\n",
    "        bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "\n",
    "    # Show the complete conversation.\n",
    "    for message in messages:\n",
    "        print(f\"Role: {message['role']}\")\n",
    "        for content in message['content']:\n",
    "            print(f\"Text: {content['text']}\")\n",
    "        print()\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Conversation with Image using Converse API </h3>\n",
    "\n",
    "In this example we will send an image as part of a message and requests that the model describe the image. The example uses Converse operation and the Amazon Nova Lite model.\n",
    "\n",
    "Sample image used in this example.\n",
    "\n",
    "![Sample Image](assets/sample_image.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_conversation(bedrock_client,\n",
    "                          model_id,\n",
    "                          input_text,\n",
    "                          input_image):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        input text : The input message.\n",
    "        input_image : The input image.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating message with model {model_id}\")\n",
    "\n",
    "    # Message to send.\n",
    "\n",
    "    with open(input_image, \"rb\") as f:\n",
    "        image = f.read()\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": input_text\n",
    "            },\n",
    "            {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'jpeg',\n",
    "                        \"source\": {\n",
    "                            \"bytes\": image\n",
    "                        }\n",
    "                    }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    messages = [message]\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model amazon.nova-lite-v1:0\n",
      "Role: assistant\n",
      "Text: There are several dogs in the image. The dogs are standing in a row in a grassy field. The dogs are of different breeds and colors. They are all looking at the camera with their tongues out. The grassy field has some pink flowers in the background. The sky is blue and clear.\n",
      "Input tokens:  729\n",
      "Output tokens:  59\n",
      "Total tokens:  788\n",
      "Stop reason: end_turn\n",
      "Finished generating text with model amazon.nova-lite-v1:0.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "input_text = \"What's in this image?\"\n",
    "input_image = \"assets/sample_image.jpg\"\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "    response = image_conversation(\n",
    "        bedrock_client, model_id, input_text, input_image)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "\n",
    "    print(f\"Role: {output_message['role']}\")\n",
    "\n",
    "    for content in output_message['content']:\n",
    "        print(f\"Text: {content['text']}\")\n",
    "\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens:  {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens:  {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens:  {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    logger.error(\"A client error occurred: %s\", message)\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conversation with Document using Converse API</h3>\n",
    "\n",
    "\n",
    "In this example, we will send a document as part of a message and requests that the model describe the contents of the document. The example uses Converse operation and the Mistral Large 2 (24.07) Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def document_conversation(bedrock_client,\n",
    "                     model_id,\n",
    "                     input_text,\n",
    "                     input_document):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        input text : The input message.\n",
    "        input_document : The input document.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating message with model {model_id}\")\n",
    "\n",
    "    # Message to send.\n",
    "    \n",
    "    with open(input_document, \"rb\") as f:\n",
    "        doc_bytes = f.read()\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": input_text\n",
    "            },\n",
    "            {\n",
    "                \"document\": {\n",
    "                    \"name\": \"MyDocument\",\n",
    "                    \"format\": \"pdf\",\n",
    "                    \"source\": {\n",
    "                        \"bytes\": doc_bytes\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    messages = [message]\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model amazon.nova-lite-v1:0\n",
      "Role: assistant\n",
      "Text: Amazon CEO Andy Jassy's 2022 letter to shareholders highlighted the company's resilience in the face of macroeconomic challenges and its commitment to long-term investments. Jassy emphasized Amazon's ability to grow demand and innovate, citing examples like the launch of AWS, Kindle, and Alexa. He discussed strategic adjustments, including cost reductions and investment in customer experience. Jassy also addressed AWS's growth, Amazon Business's expansion, and the potential of emerging segments like healthcare and Kuiper. The letter underscored Amazon's focus on customer-centric innovation and its vision for the future, including the transformative potential of Large Language Models and Generative AI. Jassy concluded with optimism about Amazon's position in the market and its potential for growth in the coming years.\n",
      "Input tokens:  16438\n",
      "Output tokens:  156\n",
      "Total tokens:  16594\n",
      "Stop reason: end_turn\n",
      "Finished generating text with model amazon.nova-lite-v1:0.\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "input_text = \"What's in this document?\"\n",
    "input_document = 'assets/2022-Shareholder-Letter.pdf'\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "    response = document_conversation(\n",
    "        bedrock_client, model_id, input_text, input_document)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "\n",
    "    print(f\"Role: {output_message['role']}\")\n",
    "\n",
    "    for content in output_message['content']:\n",
    "        print(f\"Text: {content['text']}\")\n",
    "\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens:  {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens:  {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens:  {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Next steps</h2>\n",
    "\n",
    "Now that we have seen the Converse API allow us to easily run the invocations with the same syntax across all the models, you can learn\n",
    "\n",
    "\n",
    "- How to do [function calling with the Converse API](../../agents-and-function-calling/function-calling/function_calling_with_converse/function_calling_with_converse.ipynb)\n",
    "- How to work with [Converse API and Amazon Bedrock Guardrails](../../responsible_ai/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Clean up</h2>\n",
    "\n",
    "This notebook does not require any cleanup or additional deletion of resources."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
